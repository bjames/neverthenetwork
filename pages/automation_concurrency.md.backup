title: How I Automate - Concurrency
published: 2019-08-02
category:
- Automation
author: Brandon James
summary: If you need to push configuration changes to more than one device at a time, you need concurrency.

Interacting with Network Devices can often be I/O limited. A function runs, waits for a response from the device, then another function runs so on and so forth. This is made worse by the fact that scripts are often run against multiple devices, after all the purpose of scripting is to speed up repetitive tasks. 

As an example, one of the scripts I maintain is used to test the POTS lines my enterprise uses for out-of-band connectivity at our branches and call centers. Dial-up modems are slow, you make a call, the line rings, remote end picks up, the modems eventually train up and then you finally get a connection. With some of our international locations I've seen this process take nearly a full minute. Before I added multiprocessing, this script could take over 2 hours to complete. Now it finishes in roughly 20 minutes.

As a little bit of background on my process; I typically write scripts without concurrency first using my little 3560-CX as a test device. Once the script is complete, I write a bit of logic to execute the script against multiple devices simultaneously. That logic is discussed below.

### Using the Multiprocessing Library

Python provides two easy ways to add concurrency to your scripts. The first is [threading](https://docs.python.org/3/library/threading.html) and the second is [multiprocessing](https://docs.python.org/3/library/multiprocessing.html). I use the multiprocessing primarily because it offers an extremely conveniant function called 'Pool'. There are technical reason to use one over the other. Python has someting called the Global Interpreter Lock or GIL. The GIL prevents more than one thread from controlling the Python interpeter at any given time. This means that for any CPU bound tasks, threading won't provide much of a speedup. The multiprocessing library works around this by spawning one Python process per thread. Since Network Automation is IO bound, one could use the threading library, but I don't mind the extra overhead and enjoy the convenience of the pool function.

#### functools.partial()

Partial takes a function and list of values (corresponding to the arguments the function takes) as input and then returns a new function with those arguments statically assigned.

```python
from functools import partial

def sum_four_digits(num1, num2, num3, num4):
    print num1 + num2 + num3 + num4
    
partial_one_free_variable = partial(sum_four_digits, 1, 2, 3)
partial_no_free_variables = partial(sum_four_digits, 1, 2, 3, 4)
```

Calling these two functions results in the following:

```
>>> partial_one_free_variable(0)
6
>>> partial_one_free_variable(4)
10
>>> partial_no_free_variables()
10
```

partial\_no\_free\_variables() will always use num1 = 1, num2 = 2, num3 = 3 and num4 = 4. Likewise, partial\_one\_free\_variable will always use num1 = 1, num2 = 2 and num3 = 3. In this case, partial\_one\_free\_variable ends up just adding 6 to whatever number we call it with. 

My typical use case in scripting is to create functions that already have the username and password set so I can spawn processes using multiprocessing.pool().

```python
import getpass, partial

def my_network_automation_function(script_settings, username, password):

	pass
	
if __name__ == "__main__":

	username = getpass.getuser()
	password = getpass.getpass()
	
	partial_function = partial(my_network_automation_function, username = username, password = password)
```

#### pool.map()

Map takes a function and an iterable as input, then based on the number of processes you've given it, it cuts the iterable (of length _n_) into _n/processes_ chunks and then executes them in order. Expanding on our previous example, we 